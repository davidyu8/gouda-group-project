{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prospective-romantic",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Jupyter Notebook allows the user to define and train recipe generation models from scratch. If you wish to simply create new recipes using a pre-trained model, use the Flask website instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data preparation\n",
    "import sqlite3 # connect to database .db files\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# modeling\n",
    "import pathlib # for setting up checkpoint directory\n",
    "import os # ditto\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './py')\n",
    "import train # get pre-defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-arizona",
   "metadata": {},
   "source": [
    "Set the constant `DATA_SIZE` to be the number of recipes you want to train with. We used 100,000 for training our own model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 100 # number of recipes to train on\n",
    "data_raw = train.import_data(DATA_SIZE) # acquire raw data from database\n",
    "\n",
    "# each raw recipe is in three parts; this collects them all together\n",
    "data_str = data_raw.apply(lambda x: train.condense(x.title, x.ingredients, x.instructions), axis = 1)\n",
    "\n",
    "# remove recipes that are too short (defined to be < MAX_RECIPE_LENGTH = 2000)\n",
    "data_filter = [recipe for recipe in data_str if train.filter(recipe)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters = '', # we do not want to filter our recipes\n",
    "    lower = False, # we want the model to distinguish uppercase characters\n",
    "    split = '', # we are using characters, not words\n",
    "    char_level = True # we want a character-level RNN\n",
    ")\n",
    "\n",
    "# show the tokenizer all of the existing characters we have\n",
    "tokenizer.fit_on_texts([train.STOP_SIGN])\n",
    "tokenizer.fit_on_texts(data_filter)\n",
    "\n",
    "VOCABULARY_SIZE = len(tokenizer.word_counts) + 1 # define vocabulary size\n",
    "data_vec = tokenizer.texts_to_sequences(data_filter) # vectorize the data\n",
    "\n",
    "# pad each recipe with train.STOP_SIGN until it reaches MAX_RECIPE_LENGTH\n",
    "data_temp = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences = data_vec,\n",
    "    maxlen = train.MAX_RECIPE_LENGTH - 1, # create room for stop signs at the end\n",
    "    padding = \"post\",\n",
    "    truncating = \"post\",\n",
    "    value = tokenizer.texts_to_sequences([train.STOP_SIGN])[0]\n",
    ")\n",
    "\n",
    "data_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences = data_temp,\n",
    "    maxlen = train.MAX_RECIPE_LENGTH + 1, # add on the stop signs\n",
    "    padding = \"post\",\n",
    "    truncating = \"post\",\n",
    "    value = tokenizer.texts_to_sequences([train.STOP_SIGN])[0]\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_pad) # create TensorFlow Dataset\n",
    "data_target = dataset.map(train.split_input_target) # split off first and last characters\n",
    "\n",
    "# batches the data to save memory later\n",
    "# shuffles and repeats in order to allow for infinite training (on the data end)\n",
    "data_train = data_target.shuffle(train.SHUFFLE_BUFFER_SIZE).batch(train.BATCH_SIZE, drop_remainder = True).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-example",
   "metadata": {},
   "source": [
    "# Model 1: LSTM\n",
    "\n",
    "In this section, we define and train an LSTM model. Then, we save the weights for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define location to save weights\n",
    "dir_lstm = \"weights/lstm\"\n",
    "checkpoint = os.path.join(dir_lstm, \"checkpoint_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint,\n",
    "    save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants; you can change these if you want\n",
    "EMBEDDING_SIZE = 256\n",
    "UNITS = 1024\n",
    "EPOCHS = 5\n",
    "INITIAL_EPOCH = 1\n",
    "STEPS_PER_EPOCH = 1000\n",
    "\n",
    "model_lstm = tf.keras.models.Sequential([\n",
    "  layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                   output_dim = EMBEDDING_SIZE,\n",
    "                   batch_input_shape = [train.BATCH_SIZE, None]),\n",
    "  layers.LSTM(units = UNITS,\n",
    "              return_sequences = True,\n",
    "              stateful = True,\n",
    "              recurrent_initializer = tf.keras.initializers.GlorotNormal()),\n",
    "  layers.Dense(VOCABULARY_SIZE)         \n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer = \"adam\", loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take a long time to run\n",
    "history = model_lstm.fit(\n",
    "    x = data_train,\n",
    "    epochs = EPOCHS,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    initial_epoch = INITIAL_EPOCH,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-agency",
   "metadata": {},
   "source": [
    "# Model 2: GRU\n",
    "\n",
    "In this section we develop an alternative framework, the gated recurrent unit (GRU). This type of model usually produces better results in less time. The tradeoff is that in the long run, it probably won't do as well as the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define location to save weights\n",
    "dir_gru = \"weights/gru\"\n",
    "checkpoint = os.path.join(dir_gru, \"checkpoint_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint,\n",
    "    save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "EMBEDDING_SIZE = 512\n",
    "UNITS = 1024\n",
    "\n",
    "model_gru = tf.keras.models.Sequential([\n",
    "  layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                   output_dim = EMBEDDING_SIZE,\n",
    "                   batch_input_shape = [train.BATCH_SIZE, None]),\n",
    "  layers.GRU(units = UNITS,\n",
    "             return_sequences = True,\n",
    "             stateful = True),\n",
    "  layers.Dense(VOCABULARY_SIZE)         \n",
    "])\n",
    "\n",
    "model_gru.compile(optimizer = \"adam\", loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine (if desired)\n",
    "EPOCHS = 5\n",
    "INITIAL_EPOCH = 1\n",
    "STEPS_PER_EPOCH = 1000\n",
    "\n",
    "history = model_gru.fit(\n",
    "    x = data_train,\n",
    "    epochs = 1,\n",
    "    steps_per_epoch = 1,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-bundle",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "We construct new models that mirror the previous ones, except that the batch size has been set to 1. This will allow us to load the weights of the old models and generate new recipes, one at a time.\n",
    "\n",
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine as needed\n",
    "EMBEDDING_SIZE = 256\n",
    "UNITS = 1024\n",
    "\n",
    "generator_lstm = tf.keras.models.Sequential([\n",
    "  layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                   output_dim = EMBEDDING_SIZE,\n",
    "                   batch_input_shape = [1, None]),\n",
    "  layers.LSTM(units = UNITS,\n",
    "              return_sequences = True,\n",
    "              stateful = True,\n",
    "              recurrent_initializer = tf.keras.initializers.GlorotNormal()),\n",
    "  layers.Dense(VOCABULARY_SIZE)         \n",
    "])\n",
    "\n",
    "# load weights\n",
    "generator_lstm.load_weights(tf.train.latest_checkpoint(dir_lstm)).expect_partial()\n",
    "generator_lstm.build(tf.TensorShape([1, None]))\n",
    "\n",
    "# you may see lots of warnings. Do not panic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.generate(generator_lstm, \"rice\", 100, 0.8, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-liver",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine\n",
    "EMBEDDING_SIZE = 512\n",
    "UNITS = 1024\n",
    "\n",
    "generator_gru = tf.keras.models.Sequential([\n",
    "  layers.Embedding(input_dim = VOCABULARY_SIZE,\n",
    "                   output_dim = EMBEDDING_SIZE,\n",
    "                   batch_input_shape = [1, None]),\n",
    "  layers.GRU(units = UNITS,\n",
    "              return_sequences = True,\n",
    "              stateful = True),\n",
    "  layers.Dense(VOCABULARY_SIZE)         \n",
    "])\n",
    "\n",
    "# load weights\n",
    "generator_gru.load_weights(tf.train.latest_checkpoint(dir_gru)).expect_partial()\n",
    "generator_gru.build(tf.TensorShape([1, None]))\n",
    "\n",
    "# you may see lots of warnings. Do not panic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.generate(generator_gru, \"rice\", 100, 0.8, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-framework",
   "metadata": {},
   "source": [
    "## Saving the Models\n",
    "\n",
    "Next, save each generator using the following code. Please be sure to change the filepath as needed. We used these models to produce the generator on our web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save both models\n",
    "generator_GRU.save(\"temp/gru\")\n",
    "generator_LSTM.save(\"temp/lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create new models\n",
    "generator_GRU_new = tf.keras.models.load_model(\"temp/gru\")\n",
    "generator_LSTM_new = tf.keras.models.load_model(\"temp/lstm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
