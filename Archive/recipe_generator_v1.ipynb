{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlimited-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naked-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def retrieve(word):\n",
    "    '''\n",
    "    queries recipe database for titles including a specified word\n",
    "    Parameters:\n",
    "        word (str): keyword for parsing the recipe data\n",
    "    Returns:\n",
    "        pandas DataFrame with relevant recipes\n",
    "    '''\n",
    "    \n",
    "    with sqlite3.connect(\"recipes1M.db\") as conn:\n",
    "        cmd = \\\n",
    "        f\"\"\"\n",
    "        SELECT R.title\n",
    "        FROM recipes R\n",
    "        WHERE R.title LIKE \"%{word}%\"\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(cmd, conn)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "delayed-recycling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salmon &amp; Salad a La SPORTZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Curried Pumpkin and Smoked Salmon Soup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grilled Rosemary Salmon Spedini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Garlic and Dill Salmon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spicy Grilled Orange Salmon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>Creamy Smoked Salmon and Dill Frittata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>Salmon and Spaghetti Casserole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10790</th>\n",
       "      <td>Salmon Pot Pie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10791</th>\n",
       "      <td>Pesto-Crusted Salmon Fillet With Citrus-Soy Sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10792</th>\n",
       "      <td>Cheese-and-Salmon Quesadilla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10793 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title\n",
       "0                             Salmon & Salad a La SPORTZ\n",
       "1                 Curried Pumpkin and Smoked Salmon Soup\n",
       "2                        Grilled Rosemary Salmon Spedini\n",
       "3                                 Garlic and Dill Salmon\n",
       "4                            Spicy Grilled Orange Salmon\n",
       "...                                                  ...\n",
       "10788             Creamy Smoked Salmon and Dill Frittata\n",
       "10789                     Salmon and Spaghetti Casserole\n",
       "10790                                     Salmon Pot Pie\n",
       "10791  Pesto-Crusted Salmon Fillet With Citrus-Soy Sauce\n",
       "10792                       Cheese-and-Salmon Quesadilla\n",
       "\n",
       "[10793 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = retrieve(\"salmon\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grave-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the data into the text and the ideal predicted label\n",
    "\n",
    "def give_input_split(title):\n",
    "    ''' gives the string but without the last space and the text following it'''\n",
    "    split_text = title.rsplit(\" \", maxsplit = 1)\n",
    "    return split_text[0]\n",
    "\n",
    "def give_output_split(title):\n",
    "    ''' gives the word following the last space of the text'''\n",
    "    split_text = title.rsplit(\" \", maxsplit = 1)\n",
    "    if(len(split_text) < 2):\n",
    "        return \"\"\n",
    "    return split_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aware-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns based on the previous functions\n",
    "df[\"input\"] = df[\"title\"].apply(give_input_split)\n",
    "df[\"predict\"] = df[\"title\"].apply(give_output_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "figured-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((df[\"input\"], df[\"predict\"]))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "annual-regard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Salmon & Salad a La', shape=(), dtype=string)\n",
      "tf.Tensor(b'SPORTZ', shape=(), dtype=string)\n",
      "\n",
      "tf.Tensor(b'Curried Pumpkin and Smoked Salmon', shape=(), dtype=string)\n",
      "tf.Tensor(b'Soup', shape=(), dtype=string)\n",
      "\n",
      "tf.Tensor(b'Grilled Rosemary Salmon', shape=(), dtype=string)\n",
      "tf.Tensor(b'Spedini', shape=(), dtype=string)\n",
      "\n",
      "tf.Tensor(b'Garlic and Dill', shape=(), dtype=string)\n",
      "tf.Tensor(b'Salmon', shape=(), dtype=string)\n",
      "\n",
      "tf.Tensor(b'Spicy Grilled Orange', shape=(), dtype=string)\n",
      "tf.Tensor(b'Salmon', shape=(), dtype=string)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for headline, category in data.take(5): # check the first few entries\n",
    "    print(headline)\n",
    "    print(category)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "african-international",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7555, 1079, 2159)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate data into train-test-validate\n",
    "\n",
    "data = data.shuffle(buffer_size = len(data))\n",
    "train_size = int(0.7*len(data))\n",
    "val_size   = int(0.1*len(data))\n",
    "\n",
    "train = data.take(train_size)\n",
    "val   = data.skip(train_size).take(val_size)\n",
    "test  = data.skip(train_size + val_size)\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intermediate-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(input):\n",
    "    lwer = tf.strings.lower(input)\n",
    "    punc = tf.strings.regex_replace(lwer, '[%s]' % re.escape(string.punctuation), '')\n",
    "    return punc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "innovative-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up two functions, one to vectorize input and one to vectorize the predicted values\n",
    "\n",
    "max_tokens = 5000\n",
    "sequence_length = 15\n",
    "\n",
    "vectorize_input = TextVectorization(\n",
    "    standardize = standardize,\n",
    "    max_tokens = max_tokens,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = sequence_length) \n",
    "\n",
    "vectorize_predict = TextVectorization(\n",
    "    standardize = standardize,\n",
    "    max_tokens = max_tokens,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "august-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = train.map(lambda x, y: x) # collapse the columns into a single one\n",
    "vectorize_input.adapt(recipes) # tell the vectorizers what words exist\n",
    "vectorize_predict.adapt(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "western-enough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'Smoked Salmon Tea'>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(recipes.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "random-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(), dtype=string, numpy=b'Peppered Salmon W/ Arugula (Rocket) and Yogurt Dressed'>,\n",
       "  <tf.Tensor: shape=(), dtype=string, numpy=b'Potatoes'>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "entertaining-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_recipe(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    label = tf.expand_dims(label, -1)\n",
    "    return vectorize_input(text), vectorize_predict(label)\n",
    "\n",
    "train_vec = train.map(vectorize_recipe)\n",
    "val_vec   = val.map(vectorize_recipe)\n",
    "test_vec  = test.map(vectorize_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "guilty-comparative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "  array([[53,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "        dtype=int64)>,\n",
       "  <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[185]], dtype=int64)>),\n",
       " (<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "  array([[   2,    4, 2095,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]], dtype=int64)>,\n",
       "  <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[250]], dtype=int64)>),\n",
       " (<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "  array([[ 25,   7,   2, 131,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0]], dtype=int64)>,\n",
       "  <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[1]], dtype=int64)>),\n",
       " (<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "  array([[42,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "        dtype=int64)>,\n",
       "  <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[12]], dtype=int64)>),\n",
       " (<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "  array([[ 48,  29,   2,  83, 112,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0]], dtype=int64)>,\n",
       "  <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[638]], dtype=int64)>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(val_vec.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cardiovascular-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None, 15), (None, 1)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec # shows the shape of our tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "assumed-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary1 = vectorize_input.get_vocabulary() # collect all the words used\n",
    "vocabulary2 = vectorize_predict.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "under-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_str(vec, vocab1, vocab2):\n",
    "    ''' converts numeric recipe to original title '''\n",
    "    arr1 = vec[0].numpy() # get list of numbers that make up recipe input\n",
    "    arr1 = arr1[0] # reduce dimension by one\n",
    "    arr2 = vec[1].numpy() # get number that makes up desired recipe output\n",
    "    val2 = arr2[0][0] # reduce dimensions appropriately\n",
    "    title = \"\"\n",
    "    for num in arr1: # for all numbers, we need to add on the right word\n",
    "        if num != 0: # don't want to add on spaces\n",
    "            title += vocab1[num] + \" \"\n",
    "    title += vocab2[val2]\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "confirmed-explosion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cedarplanked salmon with washington state merlot reduction and garlic spinach\n",
      "delicious pistachio encrusted salmon\n",
      "salmon steamed over orangebasil tomato sauce\n",
      "smoked salmon and horseradish cheesecake\n",
      "salmon [UNK]\n",
      "grilled salmon caesar salad\n",
      "super herbed sauteed salmon with creamy leeks and bacon\n",
      "panseared salmon puttanesca\n",
      "salmon asparagus pie\n",
      "salmon with rice [UNK]\n"
     ]
    }
   ],
   "source": [
    "sample = list(train_vec.take(10))\n",
    "\n",
    "for item in sample:\n",
    "    out = num_to_str(item, vocabulary1, vocabulary2)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work\n",
    "\n",
    "def split_input_target(title):\n",
    "    input_text = title[:-1]\n",
    "    target_text = title[1:]\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "dataset_targeted = train_vec.map(split_input_target)\n",
    "\n",
    "print(dataset_targeted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-dining",
   "metadata": {},
   "source": [
    "This code was David's attempt to transpose the data structure for the model. It was not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "extensive-premium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 25, 740,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempTensor = list(train_vec.take(1))[0][0]\n",
    "tempTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "indian-checkout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15, 1), dtype=int64, numpy=\n",
       "array([[ 25],\n",
       "       [740],\n",
       "       [  4],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0]], dtype=int64)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(tempTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "amazing-steam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(15, 1), dtype=int64, numpy=\n",
       " array([[ 107],\n",
       "        [   2],\n",
       "        [   3],\n",
       "        [2690],\n",
       "        [  19],\n",
       "        [   4],\n",
       "        [  75],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]], dtype=int64)>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = train_vec.map(lambda x, y: tf.transpose(a = x))\n",
    "list(test.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "right-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = test\n",
    "#train_vec = train_vec.map(lambda x, y: tf.transpose(a = x))\n",
    "val_vec   = val_vec.map(lambda x, y: tf.transpose(a = x))\n",
    "test_vec  = test_vec.map(lambda x, y: tf.transpose(a = x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "married-constitution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(15, 1), dtype=int64, numpy=\n",
       " array([[ 2],\n",
       "        [ 4],\n",
       "        [43],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0]], dtype=int64)>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(val_vec.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-marshall",
   "metadata": {},
   "source": [
    "This is the end of the transposing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "146f747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = max_tokens +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "joined-scratch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (15, None, 256)           1280256   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (15, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (15, None, 5001)          5126025   \n",
      "=================================================================\n",
      "Total params: 11,653,257\n",
      "Trainable params: 11,653,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        batch_input_shape=[batch_size, None]\n",
    "    ))\n",
    "\n",
    "    model.add(tf.keras.layers.LSTM(\n",
    "        units=rnn_units,\n",
    "        return_sequences=True,\n",
    "        stateful=True,\n",
    "        recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
    "    ))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size=max_tokens + 1,\n",
    "  embedding_dim=256,\n",
    "  rnn_units=1024,\n",
    "  batch_size= sequence_length\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "considered-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "extra-publisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:757 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:498 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:598 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\utils.py:79 filter_empty_gradients\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['embedding_1/embeddings:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-eddd9874a904>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 726\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:757 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:498 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:598 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    C:\\Users\\david\\Documents\\anaconda3\\envs\\PIC16B\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\utils.py:79 filter_empty_gradients\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['embedding_1/embeddings:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'].\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_vec, epochs = 10, validation_data = val_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-assurance",
   "metadata": {},
   "source": [
    "Error Text:\n",
    "InvalidArgumentError:  Input to reshape is a tensor with 15 values, but the requested shape has 225\n",
    "\t [[node gradient_tape/sequential/embedding/embedding_lookup/Reshape_1 (defined at <ipython-input-53-eddd9874a904>:1) ]] [Op:__inference_train_function_49112]\n",
    "\n",
    "Function call stack:\n",
    "train_function\n",
    "\n",
    "The main difference between our code and the fitted code is that our tensors are shaped as tuples of (None, 15) and the ideal input is supposed to be (15, None).\n",
    "\n",
    "Then if I run the transposing code, the error looks like a huge mess but the last line reads: \n",
    "\n",
    "ValueError: No gradients provided for any variable: ['embedding_1/embeddings:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'].\n",
    "\n",
    "So far I have not been able to figure out what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "irish-experiment",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'trainable_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-fd175c1d0f7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'trainable_variables'"
     ]
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-monthly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B] *",
   "language": "python",
   "name": "conda-env-PIC16B-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
