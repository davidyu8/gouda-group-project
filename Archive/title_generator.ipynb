{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "def title_generator_v1(main, aux, adj):\n",
    "    '''\n",
    "    Generates a new recipe title based on existing data.\n",
    "    Parameters:\n",
    "        main (list): list of ingredients that can be the central ingredient\n",
    "        aux (list): extra ingredients (the supporting cast)\n",
    "        adj (list): potential adjectives to describe the dish\n",
    "    Returns:\n",
    "        string object containing the new recipe title.\n",
    "    '''\n",
    "    \n",
    "    ingrMain = random.randint(0, len(main) - 1)\n",
    "    ingrAux = random.randint(0, len(aux) - 1)\n",
    "    ingrAdj = random.randint(0, len(adj) -1)\n",
    "    \n",
    "    return adj[ingrAdj] + \" \" + main[ingrMain] + \" with \" + aux[ingrAux]\n",
    "\n",
    "# create fake data\n",
    "main = [\"meatballs\", \"steak\", \"pasta\", \"lasagna\", \"ramen\", \"sandwich\"]\n",
    "aux = [\"spinach\", \"noodles\", \"rice\", \"vegetables\", \"carrots\"]\n",
    "adj = [\"curried\", \"epic\", \"roasted\", \"baked\", \"fried\", \"boiled\", \"creamed\"]\n",
    "\n",
    "new_recipe = title_generator_v1(main, aux, adj)\n",
    "print(new_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "def title_generator_v2(start):\n",
    "    '''\n",
    "    Generates a new recipe title based on bigger set of raw data.\n",
    "    Parameters:\n",
    "        start (str): keyword for parsing the recipe data\n",
    "    Returns:\n",
    "        string object with new recipe title\n",
    "    '''\n",
    "    \n",
    "    with sqlite3.connect(\"recipes1M.db\") as conn:\n",
    "        cmd = \\\n",
    "        f\"\"\"\n",
    "        SELECT R.title\n",
    "        FROM recipes R\n",
    "        WHERE R.title LIKE \"%{start}%\"\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(cmd, conn)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_generator_v2(\"meatball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn about word embeddings (a strategy for encoding text for ML)\n",
    "# https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some cleaned data\n",
    "with sqlite3.connect(\"recipes1M.db\") as conn:\n",
    "    cmd = \\\n",
    "    f\"\"\"\n",
    "    SELECT R.title\n",
    "    FROM recipes R\n",
    "    WHERE R.title LIKE \"%potato%\"\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(cmd, conn)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code was taken from\n",
    "# https://www.kdnuggets.com/2020/07/generating-cooking-recipes-using-tensorflow.html\n",
    "# and has been lightly modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = dataset.map(lambda x: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = title_generator_v2(\"salmon\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numbers\n",
    "# pass into RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfWords = []\n",
    "for i in range(0, df.shape[0]):\n",
    "    curr = df[\"title\"][i]\n",
    "    listOfWords += curr.split()\n",
    "    \n",
    "setOfWords = set(listOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = \"]\"\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level = True,\n",
    "    filters = '',\n",
    "    lower = True,\n",
    "    split = ''\n",
    ")\n",
    "\n",
    "# Stop word is not a part of recipes, but tokenizer must know about it as well.\n",
    "#tokenizer.fit_on_texts([stop])\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(listOfWords)\n",
    "\n",
    "tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = len(tokenizer.word_counts) + 1\n",
    "\n",
    "print('VOCABULARY_SIZE: ', VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_vocabulary = tokenizer.sequences_to_texts([[word_index] for word_index in range(VOCABULARY_SIZE)])\n",
    "print([char for char in array_vocabulary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vectorized = tokenizer.texts_to_sequences(listOfWords)\n",
    "\n",
    "print('Vectorized dataset size', len(dataset_vectorized)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RECIPE_LENGTH = 1000\n",
    "\n",
    "dataset_vectorized_padded_without_stops = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    dataset_vectorized,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    # We use -1 here and +1 in the next step to make sure\n",
    "    # that all recipes will have at least 1 stops sign at the end,\n",
    "    # since each sequence will be shifted and truncated afterwards\n",
    "    # (to generate X and Y sequences).\n",
    "    maxlen=MAX_RECIPE_LENGTH-1,\n",
    "    value=tokenizer.texts_to_sequences([stop])[0]\n",
    ")\n",
    "\n",
    "dataset_vectorized_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    dataset_vectorized_padded_without_stops,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    maxlen=MAX_RECIPE_LENGTH+1,\n",
    "    value=tokenizer.texts_to_sequences([stop])[0]\n",
    ")\n",
    "\n",
    "for recipe_index, recipe in enumerate(dataset_vectorized_padded[:10]):\n",
    "    print('Recipe #{} length: {}'.format(recipe_index, len(recipe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences([stop])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of code from\n",
    "# https://www.kdnuggets.com/2020/07/generating-cooking-recipes-using-tensorflow.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B] *",
   "language": "python",
   "name": "conda-env-PIC16B-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
